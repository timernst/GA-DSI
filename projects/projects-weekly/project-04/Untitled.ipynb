{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    " \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.grid_search import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Project 4: Web Scraping & Logistic Regression\n",
    "\n",
    "### Description\n",
    "\n",
    "This week we're learning about web scraping and logistic regression. Let's put these skills to the test!\n",
    "\n",
    "You're working as a data scientist for a contracting firm that's rapidly expanding. Now that they have their most valuable employee (you!), they need to leverage data to win more contracts. Your firm offers technology and scientific solutions and wants to be competitive in the hiring market. Your principal thinks the best way to gauge salary amounts is to take a look at what industry factors influence the pay scale for these professionals.\n",
    "\n",
    "Aggregators like [Indeed.com](https://www.indeed.com) regularly pool job postings from a variety of markets and industries. Your job is to understand what factors most directly impact data science salaries and effectively, accurately find appropriate data science related jobs in your metro region.\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "In this project, we will practice two major skills. Collecting data by scraping a website and then building a binary predictor with Logistic Regression.\n",
    "\n",
    "We are going to collect salary information on data science jobs in a variety of markets. Then using the location, title, and summary of the job, we will attempt to predict a corresponding salary for that job. While most listings DO NOT come with salary information (as you will see in this exercise), being to able extrapolate or predict the expected salaries for other listings will be extremely useful for negotiations :)\n",
    "\n",
    "Normally we could use regression for this task; however, instead we will convert this into a classification problem and use Logistic Regression.\n",
    "\n",
    "- **Question**: Why would we want this to be a classification problem?\n",
    "- **Answer**: While more precision may be better, there is a fair amount of natural variance in job salaries; therefore, predicting a range (e.g. high or low) may be useful.\n",
    "\n",
    "The first part of assignment will be focused on scraping data, and the second will be focused on using the listings with salary information to build a model and predict salaries.\n",
    "\n",
    "Your job is to:\n",
    "\n",
    "1. Collect data on data science salary trends from a job listings aggregator for your analysis.\n",
    "  - Select and parse data from at least ~1000 postings for jobs, potentially from multiple location searches.\n",
    "2. Find out what factors most directly impact salaries (title, location, department, etc.). In this case, we do not want to predict mean salary as would be done in a regression. Your boss believes that salary is better represented in categories than continuously\n",
    "  - Test, validate, and describe your models. What factors predict salary category? How do your models perform?\n",
    "3. Prepare a presentation for your Principal detailing your analysis.\n",
    "\n",
    "**BONUS PROBLEMS:**\n",
    "1. Your boss would rather tell a client incorrectly that they would get a lower salary job than tell a client incorrectly that they would get a high salary job. Adjust one of your logistic regression models to ease her mind, and explain what it is doing and any tradeoffs. Plot the ROC curve.\n",
    "2. Text variables and regularization:\n",
    "  - **Part 1**: Job descriptions contain more potentially useful information you could leverage. Use the job summary to find words you think would be important and add them as predictors to a model.\n",
    "  - **Part 2**: Gridsearch parameters for Ridge and Lasso for this model and report the best model.\n",
    "\n",
    "\n",
    "**Goal:** Scrape & clean data, run logistic regression, derive insights, present findings.\n",
    "\n",
    "---\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Scrape and prepare your data using BeautifulSoup.\n",
    "- A team Jupyter Notebook with your regression analysis for a peer audience of data scientists.\n",
    "- An individual blog post describing your findings, with two sections: the first for a non-technical audience, and the second for data scientist peers.\n",
    "- A five-minute in-class presentation, delivered as if you were presenting your findings to the principal who tasked you with this investigation.\n",
    "\n",
    "Please submit a link to your GitHub project repo and blog posts by the start of class on Wednesday (10/19).\n",
    "\n",
    "Presentations will be on Friday (10/21); there will be some in-class time on Thursday and Friday to prepare.\n",
    "\n",
    "---\n",
    "\n",
    "### Suggested Ways to Get Started\n",
    "\n",
    "- Read the docs for whatever technologies you use. Most of the time, there is a tutorial that you can follow, but not always, and learning to read documentation is crucial to your success!\n",
    "- Document **everything**.\n",
    "\n",
    "### Additional Resources\n",
    "- [Advice on How to Write for a Non-Technical Audience](http://programmers.stackexchange.com/questions/11523/explaining-technical-things-to-non-technical-people)\n",
    "- [Documentation for BeautifulSoup can be found here](http://www.crummy.com/software/BeautifulSoup/).\n",
    "\n",
    "---\n",
    "\n",
    "### Project Feedback + Evaluation\n",
    "\n",
    "[Attached here is a complete rubric for this project.](./project-04-rubric.md)\n",
    "\n",
    "Your instructors will score each of your technical requirements using the scale below:\n",
    "\n",
    "    Score | Expectations\n",
    "    ----- | ------------\n",
    "    **0** | _Incomplete._\n",
    "    **1** | _Does not meet expectations._\n",
    "    **2** | _Meets expectations, good job!_\n",
    "    **3** | _Exceeds expectations, you wonderful creature, you!_\n",
    "\n",
    " This will serve as a helpful overall gauge of whether you met the project goals, but __the more important scores are the individual ones__ above, which can help you identify where to focus your efforts for the next project!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title_New</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>company</th>\n",
       "      <th>ann_sal</th>\n",
       "      <th>stars</th>\n",
       "      <th>reviews</th>\n",
       "      <th>high_sal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>Oakland</td>\n",
       "      <td>CA</td>\n",
       "      <td>Even</td>\n",
       "      <td>34142.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>157</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>kWh Analytics</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>160</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>Redwood City</td>\n",
       "      <td>CA</td>\n",
       "      <td>Workbridge Associates</td>\n",
       "      <td>115000.0</td>\n",
       "      <td>41.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>199</td>\n",
       "      <td>senior data scientist</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>Workbridge Associates</td>\n",
       "      <td>165000.0</td>\n",
       "      <td>41.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>CA</td>\n",
       "      <td>Workbridge Associates</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>41.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              Title_New           city state  \\\n",
       "0         100         data scientist        Oakland    CA   \n",
       "1         157         data scientist  San Francisco    CA   \n",
       "2         160         data scientist   Redwood City    CA   \n",
       "3         199  senior data scientist  San Francisco    CA   \n",
       "4         221          data engineer  San Francisco    CA   \n",
       "\n",
       "                 company   ann_sal  stars  reviews  high_sal  \n",
       "0                   Even   34142.0    NaN      NaN         0  \n",
       "1          kWh Analytics  120000.0    NaN      NaN         1  \n",
       "2  Workbridge Associates  115000.0   41.4     25.0         1  \n",
       "3  Workbridge Associates  165000.0   41.4     25.0         1  \n",
       "4  Workbridge Associates  135000.0   41.4     25.0         1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('clean_data2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['data scientist', 'senior data scientist', 'data engineer',\n",
       "       'data other', 'data analyst', 'misc', 'scientist', 'statistician'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Title_New.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Oakland', 'San Francisco', 'Redwood City', 'Brisbane',\n",
       "       'Walnut Creek', 'Berkeley', 'Emeryville', 'South San Francisco',\n",
       "       'San Mateo', 'Foster City', 'Atlanta', 'Duluth', 'Norcross',\n",
       "       'Roswell', 'Decatur', 'Smyrna', 'Alpharetta', 'Austin', 'Cambridge',\n",
       "       'Boston', 'Wilmington', 'Norwood', 'Quincy', 'Andover', 'Maynard',\n",
       "       'Bedford', 'Hopkinton', 'Framingham', 'Malden', 'Woburn',\n",
       "       'Lexington', 'Chicago', 'Deerfield', 'Evanston', 'Des Plaines',\n",
       "       'New York', 'Long Island City', 'Manhattan', 'Elizabeth', 'Union',\n",
       "       'Orangeburg', 'Rockleigh', 'Seattle', 'Bellevue', 'Redmond',\n",
       "       'Denver', 'Boulder', 'Aurora', 'Houston', 'Chatsworth',\n",
       "       'Los Angeles', 'Santa Monica', 'Long Beach', 'Azusa', 'Pasadena',\n",
       "       'Portland', 'Vancouver', 'Research Triangle Park', 'Raleigh',\n",
       "       'Morrisville', 'Wake County', 'Raleigh-Durham', 'Durham',\n",
       "       'Salt Lake City', 'West Valley City', 'Murray', 'Greenbelt',\n",
       "       'Washington', 'McLean', 'Fort George G Meade', 'Alexandria',\n",
       "       'Largo', 'College Park', 'Rockville', 'Vienna', 'Bethesda',\n",
       "       'Bolling AFB', 'Laurel', 'Silver Spring', 'Gaithersburg',\n",
       "       'Arlington', 'Sterling', 'Springfield'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.city.unique()\n",
    "#df.state.unique()\n",
    "#df.company.unique()\n",
    "#df.ann_sal.unique()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Python2]",
   "language": "python",
   "name": "Python [Python2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
